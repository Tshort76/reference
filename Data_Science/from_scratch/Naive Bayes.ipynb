{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers attempt to divine the probability of each class given a data set.  If we represent the predicted class as a random variable $Y$ and the set of features as a random variable $X$, then naive Bayes predicts a class from data $x_i$ by selecting the class $y_i$ that have the highest value of $P(Y=y_i|X=x_i)$.  In statistical nomenclature, the class with the highest posterior probability.  So training consists of figuring out how we can estimate these posterior probabilities from a set of training data.\n",
    "\n",
    "To begin, let us assume that we are trying to predict the chance of borrower default given some historical data:\n",
    "\n",
    "| married | diet | defaulted |\n",
    "| --- | ---| ---|\n",
    "| yes | conventional | no |\n",
    "| no | conventional | yes |\n",
    "| no | vegan | yes |\n",
    "| yes | vegetarian | no |\n",
    "| no | gluten-free | no |\n",
    "| yes | West Coast | yes |\n",
    "\n",
    "\n",
    "Given this data, what is the probability that a married vegan will default? \n",
    "\n",
    "We can attempt to solve this with the standard conditional probability definition, $$P(default | (married \\cap vegan)) = \\frac{P(default \\cap married \\cap vegan)}{P(married \\cap vegan)} $$ but find that this leaves us trying to approximate the joint probability of our features.  We likely didn't gather data with this estimation in mind (we are attempting to correlate attributes with our predicted class, not other attributes).  If we assume indepedence of attributes, our denominator is always 0.  Instead, we would like to estimate $P(default | (married \\cap vegan))$ based on the relationships between our attributes and our predicted class.  We can accomplish this using Bayes' formula, which is the consequence of the following equalities:\n",
    "$$ P(Y|X) = \\frac{P(Y \\cap X)}{P(X)} $$\n",
    "$$ P(X|Y) = \\frac{P(X \\cap Y)}{P(Y)} $$\n",
    "$$P(X \\cap Y) = P(Y \\cap X)$$\n",
    "\n",
    "Solving the first two for their joint probabilities, equating them, and then dividing by $P(X)$ yields Bayes formula:\n",
    "\n",
    "$$ P(Y | X) = \\frac{P(X|Y)P(Y)}{P(X)} $$\n",
    "\n",
    "As noted above, we can make classify data $x_i$ by selecting the class $y_i$ that has the highest value of $P(Y=y_i|X=x_i)$.  Since $P(X)$ is constant across classes, we can ignore it when searching for the highest posterior.  Estimating $P(Y=y_i)$ from the training data is easy as it is just the proportion of labels that belong to a particular class.  Estimating $P(X | Y)$ is more difficult, since X is a vector of attributes and therefore $P(X=x_i)$ is shorthand for $P(x_0 = x_{i0} \\cap x_1 = x_{i1} \\cap ... \\cap x_m = x_{im})$.  With __naive bayes__, this calculation is greatly simplified by assuming that the attributes are conditionally independent.  This assumption yields:\n",
    "\n",
    "$$P((x_0 = x_{i0} \\cap x_1 = x_{i1} \\cap ... \\cap x_m = x_{im}) | Y=y) = \\prod_{j=0}^{m}{P(x_j=x_{ij} | Y=y)}$$\n",
    "\n",
    "Consequently, we can estimate our posterior (dropping the class constant denominator) as:\n",
    "\n",
    "$$P(Y=y | X=x_i) \\sim = \\prod_{j=0}^{m}{P(x_j=x_{ij} | Y=y)}*P(Y=y)$$\n",
    "\n",
    "### Training\n",
    "\n",
    "The learned 'model' is a set of prior probabilities (i.e. $P(Y=y)$ for all classes $y$) and a set of conditional probabilities (i.e. $P(x_j=x_{ij} | Y=y$).  We learn the priors based on class proportion in the training set and calculate the conditional probabilities based on the training data.\n",
    "\n",
    "### Classifying\n",
    "\n",
    "To classify, we calculate the posterior (i.e. $P(Y=y | X=x_i)$) for every class y using our learned priors and conditional probabilities.  Our label is the class with the highest posterior.\n",
    "\n",
    "### Misc\n",
    "\n",
    "Arguably the most useful characteristic of a naive bayes classifier is that it returns a probability for each class, which can be used to guage classification confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our borrower default data, calculate the naive bayes 'model' and use it to predict if a married vegan will default on their loan.\n",
    "\n",
    "---\n",
    "Our model:\n",
    "\n",
    "| model param | value |\n",
    "|---|---|\n",
    "| $P(defaulted=yes)$ | 0.5|\n",
    "| $P(defaulted=no)$ | 0.5|\n",
    "| $P(married=yes | defaulted=yes)$ | 0.333|\n",
    "| $P(married=no | defaulted=yes)$ | 0.666|\n",
    "| $P(married=yes | defaulted=no)$ | 0.666|\n",
    "| $P(married=no | defaulted=no)$ | 0.333|\n",
    "| $P(diet=conventional | defaulted=yes)$ | 0.333|\n",
    "| $P(diet=conventional | defaulted=no)$ | 0.333|\n",
    "| ... | ... |\n",
    "| $P(diet=West Coast | defaulted=yes)$ | 0.333|\n",
    "| $P(diet=West Coast | defaulted=no)$ | 0|\n",
    "\n",
    "Our posteriors:\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    " P(default=yes|(married=yes \\cap diet=vegan)) &= \\\\\n",
    " & =  P(married=yes | defaulted = yes) * P(diet=vegan | defaulted = yes) * P(defaulted=yes)\\\\\n",
    " & = 0.333 * 0.333 * 0.5 \\\\\n",
    " & = 0.055\n",
    "\\end{align}$\n",
    "\n",
    "$\\begin{align}\n",
    " P(default=no|(married=yes \\cap diet=vegan)) &= \\\\\n",
    " & =  P(married=yes | defaulted = no) * P(diet=vegan | defaulted = no) * P(defaulted=no)\\\\\n",
    " & = 0.666 * \\rho * 0.5 \\\\\n",
    " & = 0.166\\rho\n",
    "\\end{align}$\n",
    "\n",
    "We see how to actually deal with 0 conditional probabilities later but for now lets just say that $\\rho=0.333$, so we have the posterior for default=yes  is 0.055 and for default=no is 0.0833.  Therefore, our model would predict that a married vegan would not default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Edge cases\n",
    "\n",
    "#### Conditionals of zero\n",
    "As we saw in our example above, a zero conditional can occur when there is too little data (esp. with regard to a single attribute).  Since a zero conditional will zero out the product of conditionals, it is important to come up with a heuristic for dealing with them. A common approach is to use the m-estimate:\n",
    "\n",
    "$$P(x_i|y_i)=\\frac{n_c+mp}{n+m}$$\n",
    "\n",
    "where $m$ (equivalent sample size) and $p$ are parameters, $n_c$ is the number of records with attribute value $x_i$ and class $y_i$, and n is the number of records with class $y_i$.  We note that if there is no training set then $P(x_i|y_i)=p$, so $p$ can be conceived of as the prior probability of attribute value $x_i$ given class $y_i$. \n",
    "\n",
    "#### Estimating conditionals for continuous attributes\n",
    "The astute student might notice that directly calculating conditionals for continuous variables would be impossible since any single point is unlikely to occur multiple times in our training set.  The simplest way to deal with this issue is discretization; divide up the range of values into regions, map the values to regions, and treat the region 'labels' as categorical variables.  The problem with this approach is that the analyst must select the regions and this is prone to error.\n",
    "\n",
    "Alternatively, we can assume that the data fits a specific distribution (e.g. Gaussian) and use the class conditional probability of that distribution as our conditional.  Assuming a Gaussian distribution, we have:\n",
    "\n",
    "$$P(X_i=x|Y=y_j)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{ij}}e^{-\\frac{(x-\\mu_{ij})^2}{2\\sigma^2_{ij}}}$$\n",
    "\n",
    "Consequently, during the training phase we calculate the sample mean and sample variance of the attribute $X_i$'s values for records with class $y_j$ and use those values to estimate $\\mu_{ij}$ and $\\sigma_{ij}$ respectively.  At that point, we have a function for determining the conditional for a particular attribute value and class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T18:05:19.959085Z",
     "start_time": "2019-03-09T18:05:19.944903Z"
    }
   },
   "outputs": [],
   "source": [
    "#Assumptions : Only string (nominal values) or numeric fields\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class naive_bayes:\n",
    "    \n",
    "    def __init__(self, equivalent_sample_size=1, default_attr_prior=0.1):\n",
    "        self._prior = default_attr_prior\n",
    "        self._eq_sample_size = equivalent_sample_size\n",
    "       \n",
    "    \n",
    "    def _m_cond(self, class_cnt, class_attr_cnt):\n",
    "        return (class_attr_cnt + (self._eq_sample_size*self._prior)) / (class_cnt + self._eq_sample_size)\n",
    "\n",
    "\n",
    "    def _calc_priors(self, labels):\n",
    "        return labels.value_counts() / len(labels)\n",
    "\n",
    "    \n",
    "    def _categorical_conditionals(self, df):\n",
    "        \"Assumes 2 columns, attribute and label, and the attribute values are categorical\"\n",
    "        attr_col = df.columns[0]\n",
    "        class_col = df.columns[1]\n",
    "\n",
    "        conds = []\n",
    "        for y, df in data.groupby(class_col):\n",
    "            class_count = len(df)\n",
    "            conds += [[val,self._m_cond(class_count, cnt),y] for val,cnt in df[attr_col].value_counts().items()]\n",
    "\n",
    "        return pd.DataFrame(conds,columns=['Value','Probability','Class'])\n",
    "    \n",
    "    \n",
    "    def _continuous_conditionals(self, df):\n",
    "        \"Assumes 2 columns, attribute and label, and the attribute values are numeric\"\n",
    "        attr_col = df.columns[0]\n",
    "        class_col = df.columns[1]\n",
    "\n",
    "        stats = []\n",
    "        for y, df in data.groupby(class_col):\n",
    "            stats += [[df[attr_col].mean(),df[attr_col].std(),y]]\n",
    "\n",
    "        return pd.DataFrame(stats,columns=['Mean','Std','Class'])\n",
    "    \n",
    "\n",
    "    def fit(self, data,class_column):\n",
    "        result = {}\n",
    "\n",
    "        for col,is_numeric in ((data.dtypes == 'float64') | (data.dtypes == 'int64')).items():\n",
    "            if col != class_column:\n",
    "                result[col] = {'type' : 'continuous', 'conditionals' : self._continuous_conditionals(data[[col,class_column]])} if is_numeric else {'type' : 'categorical', 'conditionals' : self._categorical_conditionals(data[[col,class_column]])}\n",
    "\n",
    "        self._model = result\n",
    "        self._priors = self._calc_priors(data[class_column])\n",
    "\n",
    "\n",
    "    def _gaussian(self, val, params):\n",
    "        std = params['Std']\n",
    "        return (1.0/(math.sqrt(2*math.pi) * std)) * math.exp(-((val-params['Mean'])**2)/(2*(std**2)))\n",
    "\n",
    "    def _calc_posterior(self, record, col):\n",
    "        model_params = self._model[col]\n",
    "        x = record[col]\n",
    "        conds = model_params['conditionals']\n",
    "\n",
    "        if model_params['type'] == 'categorical':\n",
    "            return conds[conds['Value'] == x][['Probability','Class']]\n",
    "\n",
    "        #else continuous\n",
    "        gauss = lambda stats : pd.Series([self._gaussian(x,stats), stats['Class']])\n",
    "        tmp = conds.apply(gauss,axis=1)\n",
    "        return tmp.rename(columns={0 : 'Probability', 1 : 'Class'})\n",
    "\n",
    "\n",
    "    def predict(self, record):\n",
    "        if not hasattr(self, '_model'):\n",
    "            raise Exception('The model has not been fitted, prediction is impossible')\n",
    "        \n",
    "        tmp_df = pd.DataFrame()\n",
    "        for col in record.index:\n",
    "            tmp_df = tmp_df.append(self._calc_posterior(record,col),ignore_index=True)\n",
    "\n",
    "        posteriors = tmp_df.groupby('Class').prod()['Probability']*self._priors\n",
    "        print(posteriors)\n",
    "        \n",
    "        return posteriors.idxmax()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T18:05:48.553702Z",
     "start_time": "2019-03-09T18:05:48.484130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no     6.930807e-07\n",
      "yes    6.260629e-11\n",
      "dtype: float64\n",
      "no     1.902869e-08\n",
      "yes    1.020828e-06\n",
      "dtype: float64\n",
      "no     3.479887e-07\n",
      "yes    6.649849e-06\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'Diet' : ['conventional','conventional','vegan','vegetarian','gluten-free','west-coast'],\n",
    "                    'Married' : ['yes','no','no','yes','no','yes'],\n",
    "                    'Salary' : [120000.0,34000.0,54000.0,75000.0,90000.0,65000.0],\n",
    "                    'Default' : ['no','yes','yes','no','no','yes']})\n",
    "\n",
    "nb = naive_bayes()\n",
    "\n",
    "nb.fit(data,'Default')\n",
    "\n",
    "new_record = pd.Series(['east-coast','no',50000.00],index=['Diet','Married','Salary'])\n",
    "\n",
    "nb.predict(data.iloc[0,:-1])\n",
    "nb.predict(data.iloc[1,:-1])\n",
    "nb.predict(new_record)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
