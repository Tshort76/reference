{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:41:20.001736Z",
     "start_time": "2019-02-27T18:41:19.601437Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Initialize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "groceries = pd.read_csv('resources/datasets/toy_groceries.csv',index_col=0)\n",
    "\n",
    "display(groceries.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate groceries dataset\n",
    "\n",
    "# groceries_df = pd.DataFrame({'produce ($)' : [30.0,50.0,65.0,15.0,20.0],\n",
    "#                             'meat ($)' : [20.0,0.0,0.0,30.0,20.0],\n",
    "#                             'dairy ($)' : [10.0,20.0,0.0,20.0,15.0],\n",
    "#                             'other ($)' : [40.0,25.0,30.0,60.0,30.0]})\n",
    "\n",
    "\n",
    "# import random\n",
    "\n",
    "# data = [groceries_df.iloc[j]*(1 + random.random()) for i in range(10) for j in range(5)]\n",
    "\n",
    "# from datetime import date\n",
    "# from datetime import timedelta\n",
    "\n",
    "# base_date = date(2012,1,1)\n",
    "\n",
    "# dates = [base_date + timedelta(days=random.randint(1,365)) for d in data]\n",
    "\n",
    "# tmp = pd.DataFrame(data)\n",
    "    \n",
    "# tmp['loyalty-card-id'] = tmp.index\n",
    "# tmp['date'] = dates\n",
    "\n",
    "# t = tmp.reset_index(drop=True)\n",
    "\n",
    "# t.index.name = 'Transaction Id'\n",
    "\n",
    "# t.to_csv(\"resources/datasets/toy_groceries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Creating matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T15:27:24.402328Z",
     "start_time": "2019-02-21T15:27:24.311943Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "py_x = [[1,2,3,4], [5,6,7,8], [9,10,11,12]]\n",
    "\n",
    "x = np.array(py_x)\n",
    "y = np.array([[0,0,0], [1,2,3], [5,6,7], [4,8,9]])\n",
    "\n",
    "np.arange(20,40,2) #array([20, 22, 24, 26, 28, 30, 32, 34, 36, 38])\n",
    "\n",
    "np.ones([2,2])  # 2x2 matrix of 1s\n",
    "np.zeros([3,2]) # 3x2 matrix of 0s \n",
    "\n",
    "np.identity(2)  # 2x2 identity matrix\n",
    "\n",
    "np.random.randn(3,3)  # 3x3 matrix of random floats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Operations and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2],[3,4]])\n",
    "y = np.array([[5,6], [7,8]])\n",
    "\n",
    "#Arithmetic operations with scalars propagate the scalar argument to each element in the array\n",
    "\n",
    "x ** 0.5  #exponentiation of all elements\n",
    "1 / x     #multiplicative inversion of all elements\n",
    "x + 2     #addition to all elements\n",
    "\n",
    "# Any arithmetic operations between equal-size arrays applies the operation element-wise.  So we have\n",
    "\n",
    "x - y   #elementwise difference\n",
    "x + y   #elementwise sum\n",
    "x * y   #elementwise product\n",
    "x / y   #elementwise division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Some Useful Universal functions (ufuncs)\n",
    "\n",
    "#### Unary\n",
    "abs, sqrt, square, exp, log, sign, ceil, floor, rint, cos, cosh, sin, ...\n",
    "\n",
    "#### Binary\n",
    "add, subtract, multiply, divide, power, maximum, minimum, mod, >, <, & (and) , | (or), ...\n",
    "\n",
    "### Statistical Methods\n",
    "\n",
    "sum, mean, std, var, min, max, argmin, argmax, cumsum, cumprod\n",
    "\n",
    "### Set operations\n",
    "\n",
    "unique, intersect1d, union1d, setdiff1d\n",
    "\n",
    "### Linear Algebra\n",
    "\n",
    "dot, trace, det (determinant), eig (eigenvalues and eigenvectors), inv (inverse), qr (QR decomposition), svd (singular value decomposition), solve (solve the linear system Ax = b), lstsq (least-squares solution to Ax=b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Saving arrays to file\n",
    "\n",
    "Store a single array in a file\n",
    "np.save('file-name', arr)\n",
    "np.load('file-name.npy')\n",
    "\n",
    "store multipe arrays in a file\n",
    "np.savez('file-name.npz', a=arr, b=arr, ...)      keyword args will label different arrays\n",
    "my_saved_arrays = np.load('file-name.npz')\n",
    "\n",
    "my_saved_arrays['b']   will load the second array that was stored in that file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Vectorized Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random as r\n",
    "\n",
    "#Random walk with standard python\n",
    "num_steps = 1000\n",
    "position = 0\n",
    "steps = []\n",
    "\n",
    "for st in range(0,num_steps):\n",
    "    position += 1 if r.randint(0,1) else -1\n",
    "    steps.append(position)\n",
    "    \n",
    "plt.plot(steps[:100])\n",
    "plt.show()\n",
    "\n",
    "#Random walk vectorized\n",
    "# The key idea is that we construct a vector of the draws (1 or -1) and \n",
    "# then use the where function to create the history based on the steps\n",
    "\n",
    "draws = np.random.randint(0,2, size=num_steps)  #create vector of randomly occurring 0s and 1s\n",
    "\n",
    "steps = np.where(draws > 0, 1, -1)  #create a vector of 1s and -1s based on randomness from draws\n",
    "\n",
    "walk = steps.cumsum()   #create a vector of cumulative sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series\n",
    "\n",
    "A Series is best conceived of as a fixed-length, ordered dict that is is a mapping of index values to data values.\n",
    "\n",
    "### Creating Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = pd.Series([4,5,-7,0])   # index ( range(4)) is automatically created\n",
    "\n",
    "obj1.values\n",
    "obj1.index\n",
    "\n",
    "IQs = pd.Series([94,105,-7,131], index=['John','Jillian','Jesus','Tomas'])\n",
    "\n",
    "IQs = pd.Series({'John' : 94,'Jillian' : 105,'Jesus' : -7,'Tomas' : 131})\n",
    "\n",
    "#Give names to the index and the values\n",
    "\n",
    "IQs.name = 'IQ Scores'\n",
    "IQs.index.name = 'First Name'\n",
    "\n",
    "IQs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries and Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQs = pd.Series({'John' : 94,'Jillian' : 105,'Jesus' : -7,'Tomas' : 131})\n",
    "\n",
    "# Query by index or list of indices\n",
    "IQs['Tomas'] # 131\n",
    "IQs[['Tomas','John']]  # A list of indices -> A Series object containing [Thomas : 131, John : 94]\n",
    "\n",
    "#Filtering by bit mask, IQs > 0 returns index -> boolean representing if the value was > 0\n",
    "IQs[IQs > 0]\n",
    "\n",
    "# apply unary operators over values\n",
    "np.exp(IQs)\n",
    "IQs * 2\n",
    "\n",
    "# in binary operations, values are automatically aligned by index label\n",
    "yr1_revenue = pd.Series({'John' : 94,'Jillian' : 105})\n",
    "yr2_revenue = pd.Series({'John' : 100,'Jillian' : 125})\n",
    "\n",
    "yr1_revenue + yr2_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "A DataFrame can be conceived of as a fixed-length, ordered dict that is is a mapping of index values to Series objects.  It can also be thought of as an indexed 2d table of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T19:48:38.731438Z",
     "start_time": "2019-02-20T19:48:38.690374Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "#creating data frames\n",
    "\n",
    "census_pops = [[10000, 10200, 10500, 11000],\n",
    "               [10100,0 ,10500, 10700],\n",
    "               [0, 0,20500, 21700, 0, 23000]]\n",
    "\n",
    "cdata = pd.DataFrame(census_pops,columns=[\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\"], index=['Idaho',\"Nevada\",'Utah'])\n",
    "display(cdata.head())\n",
    "\n",
    "#load some toy datasets for future cells\n",
    "iris_ds = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(iris_ds.data,columns=iris_ds.feature_names)\n",
    "\n",
    "house_ds = datasets.load_boston()\n",
    "house_df = pd.DataFrame(house_ds.data,columns=house_ds.feature_names)\n",
    "\n",
    "display(iris_df.head())\n",
    "house_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Indexing and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "| Type | Notes |\n",
    "|-|-|\n",
    "|df[val] | Select single column or sequence of columns; val can be boolean array, slice operator, bit mask DataFrame |\n",
    "|df.loc[val] | select row or subset of row by _label_|\n",
    "|df.loc[val1,val2] | select both rows and columns by _label_|\n",
    "| df.iloc[idx] | select row or subset of rows based on integer position |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T19:52:49.264701Z",
     "start_time": "2019-02-20T19:52:49.252220Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_df.T   #transpose the frame\n",
    "\n",
    "iris_df.index\n",
    "iris_df.columns\n",
    "\n",
    "#get the data as a numpy array\n",
    "iris_df.values\n",
    "\n",
    "#Use iloc function when you want to do position based (as opposed to label based) slicing\n",
    "#error!!!!\n",
    "# iris_df[1]\n",
    "\n",
    "# query for the second row\n",
    "iris_df.iloc[1]\n",
    "\n",
    "#get the first 4 rows of the dframe\n",
    "iris_df.iloc[0:4]\n",
    "\n",
    "# get the first 4 columns of the dframe\n",
    "iris_df.iloc[:,0:4] \n",
    "\n",
    "# use masks\n",
    "mask = cdata['2012'] < 12000\n",
    "display(mask)\n",
    "cdata[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Operations on data frames\n",
    "\n",
    "cdata + 200 #broadcasting to add 200 to each element\n",
    "\n",
    "cdata.add(5000,fill_value=0)  #add 5000, replace NaN/None with 0 before doing so\n",
    "\n",
    "#add a row's values to each row in a data frame\n",
    "cdata + cdata.iloc[0]\n",
    "\n",
    "#add a column's value to each column\n",
    "cdata.add(cdata['2010'],axis='index')\n",
    "\n",
    "#sorting\n",
    "cdata.sort_values(by='2013',ascending=False)\n",
    "\n",
    "#sort by 2012 pop and then 2013 pop\n",
    "cdata.sort_values(by=['2012','2013'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Function|Description|\n",
    "|-|-|\n",
    "|count|Number of non-NA values|\n",
    "|describe|Compute set of summary statistics for Series or each DataFrame column|\n",
    "|min, max|Compute minimum and maximum values|\n",
    "|argmin, argmax|Compute index locations (integers) at which minimum or maximum value obtained, respectively|\n",
    "|idxmin, idxmax|Compute index labels at which minimum or maximum value obtained, respectively|\n",
    "|quantile|Compute sample quantile ranging from 0 to 1|\n",
    "|sum|Sum of values|\n",
    "|mean|Mean of values|\n",
    "|median|Arithmetic median (50% quantile) of values|\n",
    "|mad|Mean absolute deviation from mean value|\n",
    "|prod|Product of all values|\n",
    "|var|Sample variance of values|\n",
    "|std|Sample standard deviation of values|\n",
    "|skew|Sample skewness (third moment) of values|\n",
    "|kurt|Sample kurtosis (fourth moment) of values|\n",
    "|cumsum|Cumulative sum of values|\n",
    "|cummin, cummax|Cumulative minimum or maximum of values, respectively|\n",
    "|cumprod|Cumulative product of values|\n",
    "|diff|Compute first arithmetic difference (useful for time series)|\n",
    "|pct_change|Compute percent changes|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T19:42:06.157371Z",
     "start_time": "2019-02-20T19:42:06.127203Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# average value for each column\n",
    "iris_df.mean()\n",
    "\n",
    "#average value for each row\n",
    "iris_df.mean(axis='columns')\n",
    "\n",
    "#summary statistics for the data\n",
    "display(iris_df.describe())\n",
    "\n",
    "#pairwise correlation between columns\n",
    "iris_df.corr()\n",
    "\n",
    "#pairwise covariance between columns\n",
    "iris_df.cov()\n",
    "\n",
    "#correlation between each column of the DataFrame and the series argument\n",
    "iris_df.corrwith(iris_df.iloc[:,0])\n",
    "\n",
    "#get unique values in a series\n",
    "house_df['PTRATIO'].unique()\n",
    "\n",
    "#determine the frequency of each value in a series\n",
    "house_df['PTRATIO'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T21:01:57.786141Z",
     "start_time": "2019-02-20T21:01:57.773532Z"
    }
   },
   "outputs": [],
   "source": [
    "#The csv does not have a header row, need to specify columns (or pass header=None to autogenerate)\n",
    "col_names = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)','class']\n",
    "iris = pd.read_csv(\"resources/iris.csv\", names=col_names)\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file functions have lots of optional parameters to help deal with awkward input data.  Below is a sample of such functions:\n",
    "\n",
    "| Param | Use case | example |\n",
    "|-|-|-|\n",
    "| names | column names | names=['a','b','c'] |\n",
    "| index_col | which column should be used as the DataFrame's index | index_col='student_id' |\n",
    "| sep | the field delimiter using a regex | sep='\\s+' |\n",
    "| skiprows | position of rows to skip | skiprows=[0,1,2] |\n",
    "| na_values| a vector of ~= null values or a map for column specific null values | na_values=['99999999'] |\n",
    "| nrows | read the first n rows | nrows=8 |\n",
    "| chunksize | the read function will return an iterator over the file, batch size of chunksize | chunksize=1000|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the a and b columns from iris DataFrame to the 'examples/out.psv' file, using a | separator, replacing null values with the string NULL\n",
    "iris.to_csv('examples/out.psv', sep='|', na_rep='NULL', columns=['a','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON\n",
    "my_json = pd.read_json('examples/example.json') # not a working example\n",
    "my_json.to_json()  #convert the DataFrame (or Series) to JSON\n",
    "\n",
    "#HTML\n",
    "pd.read_html(\"blah.html\") #not a working example\n",
    "\n",
    "#xml - use once of the many python libraries for reading xml\n",
    "\n",
    "# Pickle (Python's binary serialization)\n",
    "iris.to_pickle('examples/pickled_iris')\n",
    "x = pd.read_pickle('examples/pickled_iris')\n",
    "\n",
    "\n",
    "#HDF5 Format - file format for storing large quantities of scientific array data\n",
    "#The HDFStore class works like a dict\n",
    "\n",
    "store = pd.HDFStore('mydata.h5')\n",
    "\n",
    "store['obj1'] = iris\n",
    "store['obj1_col'] = iris['class']\n",
    "\n",
    "#retrieve the data\n",
    "x = store['obj1']\n",
    "\n",
    "\n",
    "#excel files\n",
    "pd.ExcelFile('examples/ex1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T22:22:58.377624Z",
     "start_time": "2019-02-20T22:22:58.375675Z"
    }
   },
   "source": [
    "## Web APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T22:28:08.301847Z",
     "start_time": "2019-02-20T22:28:07.232966Z"
    }
   },
   "outputs": [],
   "source": [
    "#requests package is the go to for simplified http interaction\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "# r.status_code\n",
    "# r.headers['content-type']\n",
    "# r.encoding\n",
    "r.json()  #get the contents of the response, assuming json content-type\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This actually requires quite a bit of work, outside of this notebook, to setup your system for pyodbc ...\n",
    "# https://tryolabs.com/blog/2012/06/25/connecting-sql-server-database-python-under-ubuntu/\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "__sql_connect_phrase = ('DSN=your_dsn;'\n",
    "                        'DATABASE=your_db;'\n",
    "                        'UID=your_user;'\n",
    "                        'PWD=your_password')\n",
    "\n",
    "sql_conn = pyodbc.connect( __sql_connect_phrase)\n",
    "\n",
    "# Grab data set from sql\n",
    "all_data = pd.read_sql_query(\"SELECT * from ar.db.AccountsReceivable WHERE active='y'\", sql_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T20:50:23.093363Z",
     "start_time": "2019-02-20T20:50:23.083866Z"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# To connect to a different mongo instance, pass the mongoDB URI to MongoClient\n",
    "# format :   'mongodb://[username:password@]host1[:port1][,...hostN[:portN]]][/[database][?options]]'\n",
    "# example : mongodb://myDBReader:P40ssw0rd@mongodb0.example.com:27017/admin\n",
    "\n",
    "#checks localhost:27017 for a running mongod instance\n",
    "client = MongoClient()\n",
    "\n",
    "db = client['test'] #pdf-transforms is the database\n",
    "\n",
    "#iterates over the elements from the block-oracle collection ...\n",
    "docs = [x for x in db['temp'].find()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Missing and Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T17:18:15.731915Z",
     "start_time": "2019-02-21T17:18:15.713233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy import nan as NA\n",
    "\n",
    "s = pd.Series(['aardvark', 'artichoke', NA, 'avocado'])\n",
    "\n",
    "#boolean mask for nil values (can be used to filter data)\n",
    "s.isnull()\n",
    "\n",
    "#boolean mask for NOT nil values\n",
    "s.notnull()\n",
    "\n",
    "df = pd.DataFrame([[1., 6.0, 3.5], [1.5, NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])\n",
    "\n",
    "display(df)\n",
    "\n",
    "#drop rows\n",
    "df.dropna() #drop any row that contains a null value\n",
    "df.dropna(how='all') #drop rows that are all null\n",
    "df.dropna(thresh=2) #drop all rows that do not contain at least 2 non null values\n",
    "\n",
    "#replace nulls\n",
    "df.fillna(0)  #replace null values with 0\n",
    "df.fillna({'col1' : 0.0, 'col2' : 0.1})  #columns specific null value replacement\n",
    "df.fillna(df.mean())    #replace null values with the column mean\n",
    "\n",
    "\n",
    "#duplication\n",
    "df.duplicated() #a (series) boolean mask for duplicate rows\n",
    "df.drop_duplicates()  #drop duplicate rows\n",
    "\n",
    "#filter duplicate by k1 and k2 columns, keep last instance of duplicated record\n",
    "data.drop_duplicates(['k1', 'k2'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### map, replace, rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T20:21:48.570419Z",
     "start_time": "2019-02-21T20:21:48.542345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "display(groceries.head())\n",
    "\n",
    "#apply a function to each element of a series\n",
    "groceries['meat ($)'].map(lambda x : \"{:6.2f}\".format(x))\n",
    "\n",
    "#apply a function to each element of a dataframe\n",
    "groceries[['produce ($)','meat ($)']].applymap(lambda x : \"{:6.2f}\".format(x)).head()\n",
    "\n",
    "#replace values\n",
    "groceries.replace(999999,np.nan)\n",
    "groceries.replace(['999999','-----'],np.nan)\n",
    "groceries.replace({'99999' : np.nan, '----' : ''})\n",
    "\n",
    "#modify the index \n",
    "groceries.index.map(lambda x: 1 + x)\n",
    "\n",
    "#rename\n",
    "groceries.rename(index={0: 2}, columns={'produce ($)': 'produce'}).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "McKinney, Wes. Python for Data Analysis : Data Wrangling with Pandas, NumPy, and IPython, O'Reilly Media, Incorporated, 2017. ProQuest Ebook Central, http://ebookcentral.proquest.com/lib/boisestate/detail.action?docID=5061179."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "385687ec12c85b807b51303f5ec3a76d5987f13c66b619467696a2e6c8165e4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
