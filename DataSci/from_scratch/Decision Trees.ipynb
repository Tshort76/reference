{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:25:47.698924Z",
     "start_time": "2019-04-15T03:25:47.472297Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris_ds = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T03:25:47.706739Z",
     "start_time": "2019-04-15T03:25:47.700877Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def groupby(func,items):\n",
    "    d = {}\n",
    "    for itm in items:\n",
    "        k = func(itm)\n",
    "        if k in d:\n",
    "            d[k].append(itm)\n",
    "        else:\n",
    "            d[k] = [itm]\n",
    "    return d\n",
    "    \n",
    "# A nifty little function, mirrors Clojure's frequencies function\n",
    "def freqs(items):\n",
    "    \"Returns a dictionary of the form  {item : items_frequency}\"\n",
    "    d = {}\n",
    "    for x in items:\n",
    "        d[x] = (d[x] + 1 if x in d else 1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:33:46.128620Z",
     "start_time": "2019-03-06T02:33:46.120979Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def class_ratios(v):\n",
    "    \"Returns a list of all class ratios, helper function for impurity functions\"\n",
    "    return [b/len(v) for b in freqs(v).values()]\n",
    "\n",
    "def gini(classes):\n",
    "    return 1 - sum([x**2 for x in class_ratios(classes)])\n",
    "\n",
    "def entropy(classes):\n",
    "    return -1.0 * sum([x*math.log(x,2) for x in class_ratios(classes) if x > 0.0])\n",
    "\n",
    "#assumes parent_examples is a vector and partitions is a vector of vectors\n",
    "def gain_ratio(parent_examples, impurity_fn, partitions):\n",
    "    childrens_impurity = sum([(len(p)/len(parent_examples)) * impurity_fn(p) for p in partitions])\n",
    "    \n",
    "    split_gain = -1.0 * sum([x*math.log(x,2) for x in map(lambda a : len(a)/len(parent_examples), partitions)])\n",
    "    \n",
    "    #put 0 in here to handle a feature that has a single value repeated as every element\n",
    "    return 0 if split_gain == 0 else (impurity_fn(parent_examples) - childrens_impurity) / split_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:33:48.874868Z",
     "start_time": "2019-03-06T02:33:48.862821Z"
    }
   },
   "outputs": [],
   "source": [
    "#I created class_of and get_attr functions so that I could define an interface for the \n",
    "#rest of the code.  These two functions can deal with the complexity of the item's data layout\n",
    "#while the rest of the functions just expect a contract with these functions.  Admittedly, this is hacky.\n",
    "\n",
    "# For this assignment, I assume that an item is a vector of (unnamed) attributes and that the \n",
    "# dependent features is the last item in the vector\n",
    "def class_of(item):\n",
    "    return item[-1]\n",
    "\n",
    "#for now, we assume that attr is an index into an item\n",
    "def get_attr(attr,item):\n",
    "    return item[attr]\n",
    "\n",
    "\n",
    "#Even though I hate to use OOP (functional programming is way better), I think that it makes sense in this case\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, impurity_measure):\n",
    "        self._impurity_fnc = gini if ('gini' == impurity_measure) else entropy\n",
    "    \n",
    "    \n",
    "    def __calc_gain(self, items,partitions):\n",
    "        class_partitions = [[class_of(y) for y in p] for p in partitions.values()]\n",
    "    \n",
    "        return gain_ratio([class_of(y) for y in items], self._impurity_fnc, class_partitions)\n",
    "    \n",
    "    \n",
    "    #Enumerate all possible splits, and some meta data about those splits, for a set of binary attributes\n",
    "    def __enum_splits(self, items,attrs):\n",
    "        splits = []\n",
    "\n",
    "        for attr in attrs:\n",
    "            partitions = groupby(lambda x : get_attr(attr,x),items) #attribute vals are keys, items are vals\n",
    "            splits.append({\"partitions\" : partitions,\n",
    "                           \"split_attr\" : attr,\n",
    "                           \"gain_ratio\" : self.__calc_gain(items,partitions)})\n",
    "\n",
    "        return splits\n",
    "    \n",
    "    #Our recursive function for splitting up items based on gain ratio\n",
    "    def __make_node(self,items, attrs):\n",
    "        if (1 == len(set(map(class_of,items)))):   #homogeneous\n",
    "            return {\"class\" : class_of(items[0])}\n",
    "        elif not attrs:                            #out of attributes, choose majority\n",
    "            sorted_by_freq = sorted(freqs(map(class_of,items)).items(),key=(lambda x : x[1]))\n",
    "            return {\"class\" : sorted_by_freq[-1][0]} \n",
    "\n",
    "        #the best split is the split with the highest gain ratio\n",
    "        best_split = max(self.__enum_splits(items,attrs), \n",
    "                         key=(lambda x : x['gain_ratio']))\n",
    "\n",
    "        reproduce = lambda i,a : self.__make_node(i,[x for x in attrs if x != a])\n",
    "\n",
    "        best_split['children'] = {att_val : reproduce(itmz,best_split['split_attr']) for att_val,itmz in best_split['partitions'].items()}\n",
    "        best_split.pop('partitions', None)  #drop the partitions key, it isn't useful anymore\n",
    "\n",
    "        return best_split\n",
    "    \n",
    "    def fit(self, features, labels):\n",
    "        items = np.hstack((features,np.reshape(labels,[len(features),1])))\n",
    "        num_attributes = len(features[0])\n",
    "        self.__root = self.__make_node(items,list(range(num_attributes)))\n",
    "    \n",
    "    def __classify_recur(self, node, item):\n",
    "        if 'class' in node:\n",
    "            return node['class']\n",
    "\n",
    "        child_key = get_attr(node['split_attr'],item)\n",
    "\n",
    "        return self.__classify_recur(node['children'][child_key],item)\n",
    "    \n",
    "    def classify(self, item):\n",
    "        assert self.__root\n",
    "        return self.__classify_recur(self.__root, item)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T02:33:52.527437Z",
     "start_time": "2019-03-06T02:33:52.512004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7533333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#convert the iris data set into binary data, 1 -> greater than attribute mean, 0 -> less than or equal to mean\n",
    "# remove all classes\n",
    "\n",
    "#TODO convert to take a DataFrame and work on non binary data.  Also implement pruning\n",
    "\n",
    "deps = iris_ds.target\n",
    "tmp = iris_ds.data\n",
    "\n",
    "means = np.mean(tmp,axis=0)\n",
    "\n",
    "indeps = tmp / means\n",
    "\n",
    "indeps[indeps <= 1.0] = 0\n",
    "indeps[indeps > 1.0] = 1\n",
    "\n",
    "# indeps now contains binary data, let us build a tree\n",
    "\n",
    "#Use a different impurity metric\n",
    "tree = DecisionTree('entropy')\n",
    "\n",
    "tree.fit(indeps,deps)\n",
    "\n",
    "classified = [tree.classify(x) for x in indeps]\n",
    "\n",
    "results = [[deps[i],classified[i]] for i in range(len(deps)) if deps[i] != classified[i]]\n",
    "\n",
    "print(\"Accuracy: \",(1.0 - len(results)/len(deps)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
