{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "\n",
    "Explain why it is that we use likelihood (you can't just optimize the probability of values, we need to optimize some relationship between our model output and the training output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a technique that was developed for approximating the probability of a random variable Y taking a certain value given some experimental data.  It uses the logistic (a.k.a. sigmoid) function, $\\theta(t) =  \\frac{1}{1+e^{-t}}$, to map the results of a linear combination of the features to the range [0,1].  Summarily, we can express the probability of a random variable Y taking a value of $y_i$ given feature data $\\vec{x}$ , as:  $$ p(Y=y_i|\\vec{x}) = \\frac{1}{1+e^{-(\\vec{w}\\cdot\\vec{x} + b)}}$$\n",
    "\n",
    "In order to generate this model we note that our hypothesis fits nicely with the idea of likelihood, which is a measure of how likely a model's parameters are given some sample data.  The likelihood of the parameters of our model, $\\vec{w}$, given our training examples $\\vec{x}$ can be written as $L(\\vec{w} | \\vec{x})$.  We note that the likelihood is then the probability of our training labels given our model's output for each training example.  Now, let us derive some things more formally for the case of binary classification.\n",
    "\n",
    "Let us define the following terms:\n",
    "\n",
    "- X : A set of training examples (assuming a 1 is added to each example as the bias)\n",
    "- Y : The labels for our training examples\n",
    "- $\\vec{w}$ : The parameters for our logistic regression model\n",
    "- $ g_{\\vec{w}}(x) = \\frac{1}{1+e^{-(\\vec{w}\\cdot x)}}$\n",
    "\n",
    "\n",
    "As stated earlier, we can express $p(Y=1|X;\\vec{w}) = g_{\\vec{w}}(X)$.  Since this is a binary classification (A or not A), we can write $p(Y=0|X;\\vec{w}) = 1 - g_{\\vec{w}}(X)$.\n",
    "\n",
    "Noting that this is a Bernoulli distribution, we can use the differentiable form, $*^1$, of its the probability mass function, $f(k;p) = p^k(1-p)^{1-k} $ , to estimate $P(y|X;\\vec{w})$.  This yields $$P(y|X;\\vec{w}) = g_{\\vec{w}}(X)^y(1-g_{\\vec{w}}(X))^{1-y}$$\n",
    "\n",
    "\n",
    "With this equation we can express our likelihood as a function of our training data:\n",
    "\n",
    "\n",
    "$\\begin{align}\n",
    " L(\\vec{w} | X) & = P(Y | X;\\vec{w}) \\\\\n",
    " & = \\prod_{i=1}^{N} P(Y = y_i | x_i;\\vec{w}) \\\\\n",
    " & = \\prod_{i=1}^{N} g_{\\vec{w}}(x_i)^{y_i}(1-g_{\\vec{w}}(x_i))^{1-y_i}\n",
    "\\end{align}$\n",
    "\n",
    "Since we want to maximize likelihood and this equation would be a monster to differentiate, one often uses the log likelihood instead.  Using the properties of logarithms, we can express our log likelihood as:\n",
    "\n",
    "$\\begin{align}\n",
    " log(L(\\vec{w} | X)) & = log(\\prod_{i=1}^{N} P(Y = y_i | x_i;\\vec{w})) \\\\\n",
    " & = \\sum_{i=1}^{N} log(P(Y = y_i | x_i;\\vec{w})) \\\\\n",
    " & = \\sum_{i=1}^{N} log(g_{\\vec{w}}(x_i)^{y_i}(1-g_{\\vec{w}}(x_i))^{1-y_i}) \\\\\n",
    " & = \\sum_{i=1}^{N} log(g_{\\vec{w}}(x_i)^{y_i}) + log((1-g_{\\vec{w}}(x_i))^{1-y_i}) \\\\\n",
    " & = \\sum_{i=1}^{N} y_ilog(g_{\\vec{w}}(x_i)) + (1-y_i)log(1-g_{\\vec{w}}(x_i))\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "Before determining the gradient (derivative with respect to the weights) of the entire equation, let us derive some of the embedded gradients : \n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{d}{d\\vec{w}}g_{\\vec{w}}(x_i) & = g_{\\vec{w}}(x_i)(1-g_{\\vec{w}}(x_i))\\frac{d}{d\\vec{w}}(\\vec{w}\\cdot x) \\\\\n",
    "& = g_{\\vec{w}}(x_i)(1-g_{\\vec{w}}(x_i))x \n",
    "\\end{align}$$\n",
    "\n",
    "And the derivative for a logarithm of some function $u(\\vec{w})$:\n",
    "$$\\frac{d}{d\\vec{w}}log(u) = \\frac{u'}{u} $$\n",
    "\n",
    "Using these two building blocks, and $\\theta = g_{\\vec{w}}(x_i)$ for the sake of brevity, we can easily solve for the derivate of our log likelihood with respect to the weights $\\vec{w}$ yields:\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{d}{d\\vec{w}}log(L(\\vec{w} | X)) & = \\\\\n",
    "& = \\frac{d}{d\\vec{w}}\\sum_{i=1}^{N} y_ilog(\\theta) + (1-y_i)log(1-\\theta) \\\\\n",
    "& = \\sum_{i=1}^{N} \\frac{d}{d\\vec{w}}y_ilog(\\theta) + \\frac{d}{d\\vec{w}}(1-y_i)log(1-\\theta) \\\\\n",
    "& = \\sum_{i=1}^{N} y_i\\frac{\\theta'}{g_{\\vec{w}}(x_i)} + (1-y_i)\\frac{-\\theta'}{1 - \\theta}  \\\\\n",
    "& = \\sum_{i=1}^{N} y_i\\frac{\\theta(1-\\theta)x_i}{\\theta} + (1-y_i)\\frac{-\\theta(1-\\theta)x_i}{1 - \\theta}  \\\\\n",
    "& = \\sum_{i=1}^{N} y_i(1-\\theta)x_i + (1-y_i)(-\\theta)x_i  \\\\\n",
    "& = \\sum_{i=1}^{N} x_i(y_i-\\theta) \n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Footnotes\n",
    "\n",
    "- $*^1$ : The bernoulli probability mass function is defined as a stepwise function that take a value of $p$ for one outcome and the complementary value, $1-p$, for the other outcome.  We note that $p^k(1-p)^{1-k}$ is a clever way of writing this same thing as a continuous function, since one of the exponents will be zero for each of the 2 possible outcomes, thus reducing the probability to $p$ or $1-p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T00:20:08.111967Z",
     "start_time": "2019-03-07T00:20:08.035842Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class logistic_regression:\n",
    "    \n",
    "    def __init__(self, max_iterations=100, learning_factor=0.01):\n",
    "        self._max_iterations = max_iterations\n",
    "        self._learning_rate = learning_factor\n",
    "        self._weights = []\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if len(self._weights) > 0:\n",
    "            return 0 if sigmoid(np.dot(self._weights,np.append(x,1))) < 0.5 else 1\n",
    "        else:\n",
    "            raise Exception('The model has not been fitted, prediction is impossible')\n",
    "        \n",
    "    def fit(self, raw_features,labels):\n",
    "        features = np.append(raw_features,np.ones((len(raw_features),1)),axis=1) #add bias\n",
    "        self._weights = np.random.rand(features.shape[1])\n",
    "        \n",
    "        #calculate the gradient over all examples\n",
    "        for epoch in range(self._max_iterations):\n",
    "            predictions = sigmoid(np.dot(self._weights,features.T))\n",
    "            error = labels - predictions\n",
    "            gradient = (features.T * error).T.sum(axis=0)\n",
    "            \n",
    "            self._weights += gradient*self._learning_rate\n",
    "            \n",
    "#             print(np.sum(np.abs(error)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T00:20:10.667763Z",
     "start_time": "2019-03-07T00:20:10.411331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is mostly for my machine learning homework assignment\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris_ds = datasets.load_iris()\n",
    "\n",
    "features = iris_ds['data']\n",
    "labels = iris_ds['target']\n",
    "\n",
    "labels[labels > 1] = 1   #binary classification\n",
    "\n",
    "LR = logistic_regression(500,0.001)\n",
    "\n",
    "LR.fit(features,labels)\n",
    "\n",
    "[[LR.predict(features[i]), labels[i]] for i in range(len(features)) if LR.predict(features[i]) != labels[i]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "215.085px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
